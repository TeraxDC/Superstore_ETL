# üöÄ PROYECTO ETL - DATA LAKEHOUSE SUPERSTORE

## üéØ 1. Resumen y Objetivo del Proyecto

Este repositorio documenta la implementaci√≥n de un **Pipeline ETL (Extracci√≥n, Transformaci√≥n y Carga)** completo, dise√±ado y ejecutado en la plataforma **Azure Databricks**.

El proyecto utiliza el **Dataset Superstore** como fuente de datos y aplica la **Arquitectura Medallion (Bronze, Silver, Gold)** para demostrar las buenas pr√°cticas en la ingenier√≠a de datos, asegurando que los datos sean progresivamente limpiados, enriquecidos y optimizados para el consumo de Business Intelligence (BI).

### Tecnolog√≠as Clave:

| Tecnolog√≠a | Rol |
| :--- | :--- |
| **Databricks** | Plataforma central para el desarrollo y ejecuci√≥n del ETL. |
| **PySpark** | Lenguaje de programaci√≥n para la manipulaci√≥n y procesamiento distribuido de datos. |
| **Delta Lake** | Formato de almacenamiento utilizado en todas las capas para garantizar transacciones ACID y control de versiones. |
| **Unity Catalog** | Marco de gobernanza para la gesti√≥n centralizada de permisos y metadatos (contexto de implementaci√≥n). |

---

## üèóÔ∏è 2. Arquitectura de Datos (Medallion)

El pipeline est√° estructurado para que los datos fluyan secuencialmente a trav√©s de tres capas diferenciadas, cada una a√±adiendo valor y calidad al conjunto de datos:

| Capa | Prop√≥sito Principal | Formato / Nivel de Limpieza |
| :--- | :--- | :--- |
| **BRONZE (Raw)** | **Ingesta sin procesar.** Captura los datos crudos del Storage Account. | Datos sin modificar, solo se a√±ade una marca de tiempo de ingesta. |
| **SILVER (Cleaned/Enriched)** | **Calidad y Enriquecimiento.** Limpieza de tipos de datos, estandarizaci√≥n y *joins* de l√≥gica de negocio. | Datos limpios, mapeados a tipos de datos correctos (ej. `DateType`, `DoubleType`). |
| **GOLD (Consumed/Aggregated)** | **Data Marts.** Tablas agregadas y optimizadas, listas para reportes. | Agregaciones a nivel de negocio (ventas totales, ganancias por regi√≥n/categor√≠a). |

---

## üìÅ 3. Estructura del Repositorio

La organizaci√≥n del repositorio sigue la modularidad requerida para entornos de producci√≥n, facilitando el desarrollo, la seguridad y el despliegue continuo:
/Superstore_ETL
‚îú‚îÄ‚îÄ reversion/
‚îÇ   ‚îî‚îÄ‚îÄ rollback_scripts.sql       # Contiene scripts (SQL/PySpark) para revertir cambios de esquema o datos ante un fallo.
‚îú‚îÄ‚îÄ deploy/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.json              # Archivo de configuraci√≥n para la orquestaci√≥n (e.g., Azure Data Factory o Databricks Jobs).
‚îú‚îÄ‚îÄ seguridad/
‚îÇ   ‚îî‚îÄ‚îÄ grant_permissions.sql      # Scripts SQL para la administraci√≥n de permisos (GRANTs) sobre las tablas Gold en Unity Catalog.
‚îî‚îÄ‚îÄ proceso/
    ‚îú‚îÄ‚îÄ 1_bronze_ingestion.py      # Notebook para la ingesta de datos crudos (Capa Bronze).
    ‚îú‚îÄ‚îÄ 2_silver_transformation.py # Notebook para la limpieza y enriquecimiento de datos (Capa Silver).
    ‚îî‚îÄ‚îÄ 3_gold_aggregation.py      # Notebook para la agregaci√≥n final de datos para BI (Capa Gold).


## üêç 4. Flujo Detallado del Pipeline ETL (Notebooks PySpark)

### 4.1. `1_bronze_ingestion.py`
Este notebook gestiona la ingesta inicial desde la Capa Raw (Storage Account).

* **Fuentes:** `Orders.csv`, `Returns.csv`, `People.csv`.
* **Transformaci√≥n Clave:** La √∫nica transformaci√≥n es a√±adir la columna `_ingestion_timestamp` para el linaje de datos.
* **Destino:** Tablas Delta **`bronze_orders`**, **`bronze_returns`**, y **`bronze_people`**.

### 4.2. `2_silver_transformation.py`
Este es el paso central donde se aplica la l√≥gica de negocio y se asegura la calidad del dato.

* **Estandarizaci√≥n:** Se aplica una funci√≥n para convertir todos los nombres de columna a formato `snake_case`.
* **Validaci√≥n de Tipos:** Se corrigen expl√≠citamente las columnas de fecha (a `DateType`) y las m√©tricas (Ventas, Ganancia) a `DoubleType` o `IntegerType`.
* **L√≥gica de Negocio (Enriquecimiento):**
    * **Devoluciones:** Se realiza un **`LEFT JOIN`** entre `bronze_orders` y `bronze_returns` para etiquetar cada orden con un booleano `is_returned`.
    * **Jerarqu√≠a:** Se realiza un segundo **`LEFT JOIN`** para adjuntar la columna `regional_manager` a cada fila de orden usando la columna `region`.
* **Destino:** Tabla Delta √∫nica **`silver_orders_enriched`**.

### 4.3. `3_gold_aggregation.py`
Esta capa crea los Data Marts de alto nivel para el consumo de BI.

* **Fuente:** Tabla `silver_orders_enriched`.
* **Transformaci√≥n:** Aplicaci√≥n de funciones de agregaci√≥n (`sum`, `avg`, `countDistinct`) y funciones de tiempo (`year`, `month`).

#### Output 1: `gold_sales_by_category`
* **Agregaci√≥n:** Por `category` y `sub_category`.
* **KPIs:** `total_sales`, `total_profit`, `average_discount`, `total_orders`.

#### Output 2: `gold_regional_performance`
* **Agregaci√≥n:** Por `region`, `regional_manager`, a√±o y mes (`order_year`, `order_month`).
* **KPIs:** `total_sales`, `total_profit`, y el conteo de √≥rdenes devueltas (`returned_orders`).

---

## üíª 5. Instrucciones de Uso y Conexi√≥n BI

1.  **Ejecuci√≥n:** Los notebooks deben ser ejecutados en el orden `1_bronze` -> `2_silver` -> `3_gold` dentro de un cluster de Databricks.
2.  **Conexi√≥n BI:** Las tablas finales (**`gold_sales_by_category`** y **`gold_regional_performance`**) est√°n listas para ser consultadas directamente desde Power BI, aprovechando el conector nativo de Databricks, sin necesidad de pasos intermedios de exportaci√≥n.